{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c2282e-c165-42ca-ad7d-3a5fcfc307c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755d039-9d06-4d42-85a2-61243a30e4f9",
   "metadata": {},
   "source": [
    "# Customized Subfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eaa1be-6073-4325-ae73-dcd079a85b45",
   "metadata": {},
   "source": [
    "``AUCMEDI`` offers the possibility to implement customized subfunctions for (additional, non-standard) preprocessing of the input images.  \n",
    "\n",
    "In this notebook we will implement two customized subfunctions: a function that adds 1 to each pixel value and a function that rotates the input image.\n",
    "\n",
    "But first, we need to download the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88c76c-1c45-4091-ae1c-2c6a5a8ca137",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d74b94-c62d-4d23-8a8f-c794ef4d87e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 12:35:09.514247: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "cwd = !pwd\n",
    "datadir = cwd[0] + \"/data\"\n",
    "Path(datadir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#print('Beginning file download with wget module')\n",
    "\n",
    "#url = 'https://zenodo.org/record/53169/files/Kather_texture_2016_image_tiles_5000.zip?download=1'\n",
    "#wget.download(url, datadir)\n",
    "\n",
    "#with zipfile.ZipFile(\"data/Kather_texture_2016_image_tiles_5000.zip\",\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(\"data\")\n",
    "\n",
    "from aucmedi.data_processing.io_data import input_interface\n",
    "ds_loader = input_interface(\"directory\", path_imagedir=\"data/Kather_texture_2016_image_tiles_5000\", path_data=None, training=True, ohe=False)\n",
    "(samples, class_ohe, nclasses, class_names, image_format) = ds_loader\n",
    "\n",
    "from aucmedi.sampling.split import sampling_split\n",
    "train, validation, test = sampling_split(samples, class_ohe, sampling=[0.5, 0.25, 0.25], \n",
    "                                         stratified=True, iterative=False, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfa58e-a3e5-4140-b5cf-abf8dc0c7d2f",
   "metadata": {},
   "source": [
    "## Define the customized Subfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d33312-58e4-45ca-b661-d4196f64344b",
   "metadata": {},
   "source": [
    "Now we are going to define our customized subfunctions.  \n",
    "\n",
    "All subfunctions are based on the abstract base class [Subfunction_Base](https://frankkramer-lab.github.io/aucmedi/reference/data_processing/subfunctions/sf_base/#aucmedi.data_processing.subfunctions.sf_base.Subfunction_Base).\n",
    "\n",
    "All classes that are derived from the abstract class [Subfunction_Base](https://frankkramer-lab.github.io/aucmedi/reference/data_processing/subfunctions/sf_base/#aucmedi.data_processing.subfunctions.sf_base.Subfunction_Base) are required to have the functions ``__init__()`` and ``transform()``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd30507-0ed6-4cb8-9a2d-d4f903688532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aucmedi.data_processing.subfunctions.sf_base import Subfunction_Base\n",
    "import numpy as np\n",
    "\n",
    "class plus_one(Subfunction_Base):\n",
    "    def __init__(self): \n",
    "        pass\n",
    "\n",
    "    def transform(self, image):\n",
    "        new_images = np.where(image < 255, image, + 1.0) \n",
    "        return new_images                \n",
    "\n",
    "    \n",
    "class rotate(Subfunction_Base):\n",
    "    def __init__(self):    \n",
    "        pass\n",
    "    \n",
    "    def transform(self, image):\n",
    "        new_images = np.rot90(image, 2)    # rotate 180 degrees\n",
    "        return new_images   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035dc6e-66ec-4050-856f-3eb48facc667",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea1a78-aee5-474f-875c-a1ad9f506c90",
   "metadata": {},
   "source": [
    "The definition of the ``NeuralNetwork`` works as usual. If you have questions, have a look in the corresponding notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164f9815-d621-4b77-8435-97b67b01f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 12:35:12.469111: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-14 12:35:13.027895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22844 MB memory:  -> device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:3f:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from aucmedi.neural_network.model import NeuralNetwork\n",
    "import tensorflow.keras as tfa\n",
    "\n",
    "f1Score = tfa.metrics.F1Score(num_classes=nclasses, threshold=0.5)\n",
    "\n",
    "model = NeuralNetwork(n_labels=nclasses, channels=3, architecture=\"2D.ResNet50\", \n",
    "                      loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\", f1Score], \n",
    "                      activation_output=\"softmax\", pretrained_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153af83-ac97-4c4d-9dda-69c99b337cc3",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e99e44e-1665-40fc-b91b-0797e3ba7d9d",
   "metadata": {},
   "source": [
    "Now we created batches for training with the `DataGenerator`.  \n",
    "This is, were we can call our subfunction. We tell the ``DataGenerator`` with the ``subfunction``s-argument to apply the transform-method of the ``plus_one()`` and the ``rotate()`` object on each image. (The objects have to be put is a list, even if it is only one object.)\n",
    "\n",
    "Next, the model can be trained as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c9dec3-8156-470d-b454-8cbe03db48cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 12:35:19.992075: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-08-14 12:35:20.536104: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 23s 201ms/step - loss: 1.9261 - categorical_accuracy: 0.4420 - f1_score: 0.4283 - val_loss: 3.3297 - val_categorical_accuracy: 0.1248 - val_f1_score: 0.0279\n",
      "Epoch 2/20\n",
      "79/79 [==============================] - 15s 188ms/step - loss: 1.3214 - categorical_accuracy: 0.5984 - f1_score: 0.5840 - val_loss: 8.4594 - val_categorical_accuracy: 0.1248 - val_f1_score: 0.0277\n",
      "Epoch 3/20\n",
      "79/79 [==============================] - 15s 188ms/step - loss: 1.0543 - categorical_accuracy: 0.6444 - f1_score: 0.6299 - val_loss: 5.6121 - val_categorical_accuracy: 0.1760 - val_f1_score: 0.0717\n",
      "Epoch 4/20\n",
      "79/79 [==============================] - 14s 180ms/step - loss: 1.0774 - categorical_accuracy: 0.6520 - f1_score: 0.6446 - val_loss: 2.1746 - val_categorical_accuracy: 0.4784 - val_f1_score: 0.4117\n",
      "Epoch 5/20\n",
      "79/79 [==============================] - 15s 194ms/step - loss: 0.9084 - categorical_accuracy: 0.7064 - f1_score: 0.7075 - val_loss: 2.2169 - val_categorical_accuracy: 0.5808 - val_f1_score: 0.5203\n",
      "Epoch 6/20\n",
      "79/79 [==============================] - 14s 180ms/step - loss: 0.7911 - categorical_accuracy: 0.7312 - f1_score: 0.7264 - val_loss: 1.7152 - val_categorical_accuracy: 0.4976 - val_f1_score: 0.4504\n",
      "Epoch 7/20\n",
      "79/79 [==============================] - 16s 196ms/step - loss: 0.7942 - categorical_accuracy: 0.7428 - f1_score: 0.7398 - val_loss: 2.9273 - val_categorical_accuracy: 0.6200 - val_f1_score: 0.5542\n",
      "Epoch 8/20\n",
      "79/79 [==============================] - 16s 199ms/step - loss: 0.6206 - categorical_accuracy: 0.7920 - f1_score: 0.7890 - val_loss: 4.0862 - val_categorical_accuracy: 0.5488 - val_f1_score: 0.4577\n",
      "Epoch 9/20\n",
      "79/79 [==============================] - 15s 188ms/step - loss: 0.5090 - categorical_accuracy: 0.8240 - f1_score: 0.8219 - val_loss: 3.6419 - val_categorical_accuracy: 0.6040 - val_f1_score: 0.5657\n",
      "Epoch 10/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.5886 - categorical_accuracy: 0.8000 - f1_score: 0.8006 - val_loss: 1.3022 - val_categorical_accuracy: 0.6264 - val_f1_score: 0.5766\n",
      "Epoch 11/20\n",
      "79/79 [==============================] - 15s 189ms/step - loss: 0.4838 - categorical_accuracy: 0.8484 - f1_score: 0.8463 - val_loss: 1.0787 - val_categorical_accuracy: 0.6720 - val_f1_score: 0.6205\n",
      "Epoch 12/20\n",
      "79/79 [==============================] - 16s 197ms/step - loss: 0.3381 - categorical_accuracy: 0.8860 - f1_score: 0.8848 - val_loss: 4.7745 - val_categorical_accuracy: 0.4968 - val_f1_score: 0.3954\n",
      "Epoch 13/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.3090 - categorical_accuracy: 0.8900 - f1_score: 0.8886 - val_loss: 1.3746 - val_categorical_accuracy: 0.6592 - val_f1_score: 0.6101\n",
      "Epoch 14/20\n",
      "79/79 [==============================] - 15s 191ms/step - loss: 0.3421 - categorical_accuracy: 0.8876 - f1_score: 0.8874 - val_loss: 7.7929 - val_categorical_accuracy: 0.6848 - val_f1_score: 0.6314\n",
      "Epoch 15/20\n",
      "79/79 [==============================] - 15s 194ms/step - loss: 0.3236 - categorical_accuracy: 0.8892 - f1_score: 0.8905 - val_loss: 3.3072 - val_categorical_accuracy: 0.5280 - val_f1_score: 0.4954\n",
      "Epoch 16/20\n",
      "79/79 [==============================] - 16s 196ms/step - loss: 0.2997 - categorical_accuracy: 0.9016 - f1_score: 0.9015 - val_loss: 2.0916 - val_categorical_accuracy: 0.5736 - val_f1_score: 0.5273\n",
      "Epoch 17/20\n",
      "79/79 [==============================] - 15s 184ms/step - loss: 0.3253 - categorical_accuracy: 0.9028 - f1_score: 0.9026 - val_loss: 4.7350 - val_categorical_accuracy: 0.5632 - val_f1_score: 0.5121\n",
      "Epoch 18/20\n",
      "79/79 [==============================] - 15s 184ms/step - loss: 0.2407 - categorical_accuracy: 0.9244 - f1_score: 0.9248 - val_loss: 1.5802 - val_categorical_accuracy: 0.6968 - val_f1_score: 0.7064\n",
      "Epoch 19/20\n",
      "79/79 [==============================] - 15s 188ms/step - loss: 0.3023 - categorical_accuracy: 0.9108 - f1_score: 0.9117 - val_loss: 5.2041 - val_categorical_accuracy: 0.6920 - val_f1_score: 0.6424\n",
      "Epoch 20/20\n",
      "79/79 [==============================] - 15s 189ms/step - loss: 0.2239 - categorical_accuracy: 0.9312 - f1_score: 0.9320 - val_loss: 4.0974 - val_categorical_accuracy: 0.6536 - val_f1_score: 0.6122\n"
     ]
    }
   ],
   "source": [
    "from aucmedi.data_processing.data_generator import DataGenerator\n",
    "\n",
    "train_generator = DataGenerator(samples=train[0], path_imagedir=\"data/Kather_texture_2016_image_tiles_5000\",\n",
    "                                               resize=model.meta_input, standardize_mode=model.meta_standardize,\n",
    "                                               labels=train[1], image_format=image_format, batch_size=32, data_aug=None, \n",
    "                                               grayscale=False, prepare_images=False, subfunctions=[plus_one(), rotate()],\n",
    "                                               sample_weights=None, seed=123, workers=1)\n",
    "val_generator = DataGenerator(samples=validation[0], path_imagedir=\"data/Kather_texture_2016_image_tiles_5000\",\n",
    "                                             resize=model.meta_input, standardize_mode=model.meta_standardize,\n",
    "                                             labels=validation[1], image_format=image_format, batch_size=32, data_aug=None, \n",
    "                                             grayscale=False, prepare_images=False, subfunctions=[plus_one(), rotate()],\n",
    "                                             sample_weights=None, seed=123, workers=1)\n",
    "\n",
    "history = model.train(training_generator=train_generator, validation_generator=val_generator, epochs=20, iterations=None, \n",
    "                                         callbacks=None, class_weights=None, transfer_learning=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
